# PIPELINE DEFINITION
# Name: adult-income-end2end
# Inputs:
#    dataset_url: str [Default: 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult']
#    random_state: int [Default: 42.0]
#    train_epochs: int [Default: 20.0]
#    trials: int [Default: 10.0]
#    tune_epochs: int [Default: 5.0]
# Outputs:
#    train-eval-torch-op-metrics_out: system.Metrics
#    tune-op-tuning_metrics: system.Metrics
components:
  comp-preprocess-op:
    executorLabel: exec-preprocess-op
    inputDefinitions:
      parameters:
        dataset_url:
          parameterType: STRING
        random_state:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        test_size:
          defaultValue: 0.2
          isOptional: true
          parameterType: NUMBER_DOUBLE
        val_size:
          defaultValue: 0.1
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        output_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-eval-torch-op:
    executorLabel: exec-train-eval-torch-op
    inputDefinitions:
      artifacts:
        best_params:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        epochs:
          defaultValue: 20.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        patience:
          defaultValue: 5.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        seed:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        weight_decay:
          defaultValue: 0.0001
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        metrics_out:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-tune-op:
    executorLabel: exec-tune-op
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        epochs:
          defaultValue: 5.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        seed:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        trials:
          defaultValue: 10.0
          isOptional: true
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      artifacts:
        best_params:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        tuning_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-preprocess-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy==1.26.4'\
          \ 'scikit-learn==1.4.2' 'joblib==1.4.2' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess_op(\n    dataset_url: str,\n    output_dataset: Output[Dataset],\n\
          \    test_size: float = 0.2,\n    val_size: float = 0.1,\n    random_state:\
          \ int = 42,\n) -> None:\n    import csv\n    import os\n    import urllib.request\n\
          \    import numpy as np\n    from sklearn.model_selection import train_test_split\n\
          \    from sklearn.compose import ColumnTransformer\n    from sklearn.preprocessing\
          \ import OneHotEncoder, StandardScaler\n    from sklearn.pipeline import\
          \ Pipeline\n    import joblib\n\n    adult_data_url = dataset_url.rstrip(\"\
          /\") + \"/adult.data\"\n    adult_test_url = dataset_url.rstrip(\"/\") +\
          \ \"/adult.test\"\n\n    def _download_text(url: str) -> str:\n        with\
          \ urllib.request.urlopen(url) as resp:\n            return resp.read().decode(\"\
          utf-8\", errors=\"replace\")\n\n    def _parse_adult_csv(text: str, is_test:\
          \ bool) -> list[list[str]]:\n        lines: list[str] = []\n        for\
          \ i, raw in enumerate(text.splitlines()):\n            raw = raw.strip()\n\
          \            if not raw or raw.startswith(\"|\"):\n                continue\n\
          \            if is_test and i == 0 and raw.lower().startswith(\"age\"):\n\
          \                continue\n            lines.append(raw)\n\n        rows:\
          \ list[list[str]] = []\n        reader = csv.reader(lines, delimiter=\"\
          ,\", skipinitialspace=True)\n        for row in reader:\n            if\
          \ len(row) != 15:\n                continue\n            if is_test:\n \
          \               row[-1] = row[-1].rstrip(\".\")\n            rows.append(row)\n\
          \        return rows\n\n    data_text = _download_text(adult_data_url)\n\
          \    test_text = _download_text(adult_test_url)\n\n    train_rows = _parse_adult_csv(data_text,\
          \ is_test=False)\n    test_rows = _parse_adult_csv(test_text, is_test=True)\n\
          \n    all_rows = train_rows + test_rows\n    X_raw = [r[:-1] for r in all_rows]\n\
          \    y_raw = [r[-1] for r in all_rows]\n\n    y = np.array([1 if v.strip()\
          \ == \">50K\" else 0 for v in y_raw], dtype=np.int64)\n\n    numeric_idx\
          \ = [0, 2, 4, 10, 11, 12]\n    categorical_idx = [1, 3, 5, 6, 7, 8, 9, 13]\n\
          \n    preprocessor = ColumnTransformer(\n        transformers=[\n      \
          \      (\"num\", Pipeline(steps=[(\"scaler\", StandardScaler())]), numeric_idx),\n\
          \            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\
          \ categorical_idx),\n        ],\n        remainder=\"drop\",\n    )\n\n\
          \    X_obj = np.array(X_raw, dtype=object)\n    X = preprocessor.fit_transform(X_obj).astype(np.float32)\n\
          \n    X_train, X_tmp, y_train, y_tmp = train_test_split(\n        X, y,\n\
          \        test_size=(test_size + val_size),\n        random_state=random_state,\n\
          \        stratify=y,\n    )\n\n    val_frac = val_size / max(1e-9, (val_size\
          \ + test_size))\n    X_val, X_test, y_val, y_test = train_test_split(\n\
          \        X_tmp, y_tmp,\n        test_size=(1.0 - val_frac),\n        random_state=random_state,\n\
          \        stratify=y_tmp,\n    )\n\n    os.makedirs(output_dataset.path,\
          \ exist_ok=True)\n    np.savez_compressed(\n        os.path.join(output_dataset.path,\
          \ \"splits.npz\"),\n        X_train=X_train, y_train=y_train,\n        X_val=X_val,\
          \ y_val=y_val,\n        X_test=X_test, y_test=y_test,\n    )\n    joblib.dump(preprocessor,\
          \ os.path.join(output_dataset.path, \"preprocessor.joblib\"))\n\n"
        image: python:3.11-slim
    exec-train-eval-torch-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_eval_torch_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy==1.26.4'\
          \ 'scikit-learn==1.4.2' 'joblib==1.4.2' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_eval_torch_op(\n    dataset: Input[Dataset],\n    best_params:\
          \ Input[Artifact],\n    model: Output[Model],            # \u2705 \u043E\
          \u0432\u0430 \u043C\u043E\u0440\u0430 \u0434\u0430 \u0435 Model\n    metrics_out:\
          \ Output[Metrics],\n    epochs: int = 20,\n    weight_decay: float = 1e-4,\n\
          \    patience: int = 5,\n    seed: int = 42,\n) -> None:\n    import os,\
          \ json, numpy as np\n    _install_torch_cpu()\n\n    import torch\n    import\
          \ torch.nn as nn\n    from torch.utils.data import TensorDataset, DataLoader\n\
          \    from sklearn.metrics import accuracy_score, f1_score, log_loss\n\n\
          \    def set_seed(s: int) -> None:\n        import random\n        random.seed(s);\
          \ np.random.seed(s); torch.manual_seed(s)\n\n    set_seed(seed)\n    device\
          \ = torch.device(\"cpu\")\n\n    with open(os.path.join(best_params.path,\
          \ \"best_params.json\"), \"r\", encoding=\"utf-8\") as f:\n        bp =\
          \ json.load(f)\n\n    lr = float(bp[\"lr\"])\n    hidden = int(bp[\"hidden\"\
          ])\n    dropout = float(bp[\"dropout\"])\n    batch_size = int(bp[\"batch_size\"\
          ])\n\n    data = np.load(os.path.join(dataset.path, \"splits.npz\"))\n \
          \   X_train = data[\"X_train\"].astype(np.float32)\n    y_train = data[\"\
          y_train\"].astype(np.float32)\n    X_val = data[\"X_val\"].astype(np.float32)\n\
          \    y_val = data[\"y_val\"].astype(np.int64)\n    X_test = data[\"X_test\"\
          ].astype(np.float32)\n    y_test = data[\"y_test\"].astype(np.int64)\n\n\
          \    Xtr = torch.from_numpy(X_train)\n    ytr = torch.from_numpy(y_train)\n\
          \    Xv = torch.from_numpy(X_val)\n    yv = torch.from_numpy(y_val.astype(np.float32))\n\
          \    Xt = torch.from_numpy(X_test)\n\n    in_dim = X_train.shape[1]\n\n\
          \    class MLP(nn.Module):\n        def __init__(self, d_in: int, d_hidden:\
          \ int, p_drop: float):\n            super().__init__()\n            self.net\
          \ = nn.Sequential(\n                nn.Linear(d_in, d_hidden), nn.ReLU(),\
          \ nn.Dropout(p_drop),\n                nn.Linear(d_hidden, d_hidden), nn.ReLU(),\
          \ nn.Dropout(p_drop),\n                nn.Linear(d_hidden, 1),\n       \
          \     )\n        def forward(self, x): return self.net(x).squeeze(-1)\n\n\
          \    net = MLP(in_dim, hidden, dropout).to(device)\n    opt = torch.optim.Adam(net.parameters(),\
          \ lr=lr, weight_decay=weight_decay)\n    loss_fn = nn.BCEWithLogitsLoss()\n\
          \n    train_loader = DataLoader(TensorDataset(Xtr, ytr), batch_size=batch_size,\
          \ shuffle=True)\n    val_loader = DataLoader(TensorDataset(Xv, yv), batch_size=batch_size,\
          \ shuffle=False)\n\n    best_val_loss = float(\"inf\")\n    best_state =\
          \ None\n    bad_epochs = 0\n    last_val_acc = 0.0\n    last_val_f1 = 0.0\n\
          \n    for _ in range(epochs):\n        net.train()\n        for xb, yb in\
          \ train_loader:\n            opt.zero_grad(set_to_none=True)\n         \
          \   loss = loss_fn(net(xb), yb)\n            loss.backward()\n         \
          \   opt.step()\n\n        net.eval()\n        with torch.no_grad():\n  \
          \          v_logits = torch.cat([net(xb).cpu() for xb, _ in val_loader],\
          \ dim=0).numpy()\n            v_y = torch.cat([yb.cpu() for _, yb in val_loader],\
          \ dim=0).numpy()\n        v_true = (v_y >= 0.5).astype(int)\n\n        v_proba\
          \ = 1.0 / (1.0 + np.exp(-v_logits))\n        v_pred = (v_proba >= 0.5).astype(int)\n\
          \n        val_acc = float(accuracy_score(v_true, v_pred))\n        val_f1\
          \ = float(f1_score(v_true, v_pred))\n        val_ll = float(log_loss(v_true,\
          \ v_proba))\n\n        last_val_acc = val_acc\n        last_val_f1 = val_f1\n\
          \n        if val_ll + 1e-6 < best_val_loss:\n            best_val_loss =\
          \ val_ll\n            best_state = {k: v.detach().cpu().clone() for k, v\
          \ in net.state_dict().items()}\n            bad_epochs = 0\n        else:\n\
          \            bad_epochs += 1\n            if bad_epochs >= patience:\n \
          \               break\n\n    if best_state is not None:\n        net.load_state_dict(best_state)\n\
          \n    net.eval()\n    with torch.no_grad():\n        t_logits = net(Xt).cpu().numpy()\n\
          \n    t_proba = 1.0 / (1.0 + np.exp(-t_logits))\n    t_pred = (t_proba >=\
          \ 0.5).astype(int)\n\n    test_acc = float(accuracy_score(y_test, t_pred))\n\
          \    test_f1 = float(f1_score(y_test, t_pred))\n\n    metrics_out.log_metric(\"\
          val_log_loss_best\", float(best_val_loss))\n    metrics_out.log_metric(\"\
          val_accuracy\", float(last_val_acc))\n    metrics_out.log_metric(\"val_f1\"\
          , float(last_val_f1))\n    metrics_out.log_metric(\"test_accuracy\", float(test_acc))\n\
          \    metrics_out.log_metric(\"test_f1\", float(test_f1))\n\n    # \u2705\
          \ save as model\n    os.makedirs(model.path, exist_ok=True)\n    example\
          \ = torch.zeros((1, in_dim), dtype=torch.float32).to(device)\n    scripted\
          \ = torch.jit.trace(net, example)\n    scripted.save(os.path.join(model.path,\
          \ \"model.pt\"))\n\n"
        image: python:3.11-slim
    exec-tune-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - tune_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy==1.26.4'\
          \ 'scikit-learn==1.4.2' 'joblib==1.4.2' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef tune_op(\n    dataset: Input[Dataset],\n    tuning_metrics: Output[Metrics],\n\
          \    best_params: Output[Artifact],\n    trials: int = 10,\n    epochs:\
          \ int = 5,\n    seed: int = 42,\n) -> None:\n    import json, os, random\n\
          \    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n\
          \n    lr_space = [0.0005, 0.001, 0.002, 0.003]\n    hidden_space = [64,\
          \ 128, 256]\n    dropout_space = [0.0, 0.1, 0.2, 0.3]\n    batch_space =\
          \ [128, 256, 512]\n\n    _install_torch_cpu()\n    import torch\n    import\
          \ torch.nn as nn\n    from torch.utils.data import TensorDataset, DataLoader\n\
          \    from sklearn.metrics import accuracy_score, f1_score, log_loss\n\n\
          \    data = np.load(os.path.join(dataset.path, \"splits.npz\"))\n    X_train\
          \ = data[\"X_train\"].astype(np.float32)\n    y_train = data[\"y_train\"\
          ].astype(np.float32)\n    X_val = data[\"X_val\"].astype(np.float32)\n \
          \   y_val = data[\"y_val\"].astype(np.int64)\n\n    Xtr = torch.from_numpy(X_train)\n\
          \    ytr = torch.from_numpy(y_train)\n    Xv = torch.from_numpy(X_val)\n\
          \n    in_dim = X_train.shape[1]\n    device = torch.device(\"cpu\")\n\n\
          \    class MLP(nn.Module):\n        def __init__(self, d_in: int, d_hidden:\
          \ int, p_drop: float):\n            super().__init__()\n            self.net\
          \ = nn.Sequential(\n                nn.Linear(d_in, d_hidden), nn.ReLU(),\
          \ nn.Dropout(p_drop),\n                nn.Linear(d_hidden, d_hidden), nn.ReLU(),\
          \ nn.Dropout(p_drop),\n                nn.Linear(d_hidden, 1),\n       \
          \     )\n        def forward(self, x): return self.net(x).squeeze(-1)\n\n\
          \    best = None\n    best_score = -1.0\n\n    for t in range(1, trials\
          \ + 1):\n        lr = random.choice(lr_space)\n        hidden = random.choice(hidden_space)\n\
          \        dropout = random.choice(dropout_space)\n        batch_size = random.choice(batch_space)\n\
          \n        net = MLP(in_dim, hidden, dropout).to(device)\n        opt = torch.optim.Adam(net.parameters(),\
          \ lr=lr)\n        loss_fn = nn.BCEWithLogitsLoss()\n\n        train_loader\
          \ = DataLoader(TensorDataset(Xtr, ytr), batch_size=batch_size, shuffle=True)\n\
          \n        for _ in range(epochs):\n            net.train()\n           \
          \ for xb, yb in train_loader:\n                opt.zero_grad(set_to_none=True)\n\
          \                loss = loss_fn(net(xb), yb)\n                loss.backward()\n\
          \                opt.step()\n\n        net.eval()\n        with torch.no_grad():\n\
          \            v_logits = net(Xv).cpu().numpy()\n\n        v_proba = 1.0 /\
          \ (1.0 + np.exp(-v_logits))\n        v_pred = (v_proba >= 0.5).astype(int)\n\
          \n        val_acc = float(accuracy_score(y_val, v_pred))\n        val_f1\
          \ = float(f1_score(y_val, v_pred))\n        val_ll = float(log_loss(y_val,\
          \ v_proba))\n\n        print(f\"trial={t}/{trials} val_accuracy={val_acc:.6f}\
          \ val_f1={val_f1:.6f} val_log_loss={val_ll:.6f}\")\n\n        if val_f1\
          \ > best_score:\n            best_score = val_f1\n            best = {\"\
          lr\": lr, \"hidden\": hidden, \"dropout\": dropout, \"batch_size\": batch_size,\n\
          \                    \"val_accuracy\": val_acc, \"val_f1\": val_f1, \"val_log_loss\"\
          : val_ll}\n\n    os.makedirs(best_params.path, exist_ok=True)\n    with\
          \ open(os.path.join(best_params.path, \"best_params.json\"), \"w\", encoding=\"\
          utf-8\") as f:\n        json.dump(best, f, indent=2)\n\n    tuning_metrics.log_metric(\"\
          best_val_accuracy\", float(best[\"val_accuracy\"]))\n    tuning_metrics.log_metric(\"\
          best_val_f1\", float(best[\"val_f1\"]))\n    tuning_metrics.log_metric(\"\
          best_val_log_loss\", float(best[\"val_log_loss\"]))\n\n"
        image: python:3.11-slim
pipelineInfo:
  name: adult-income-end2end
root:
  dag:
    outputs:
      artifacts:
        train-eval-torch-op-metrics_out:
          artifactSelectors:
          - outputArtifactKey: metrics_out
            producerSubtask: train-eval-torch-op
        tune-op-tuning_metrics:
          artifactSelectors:
          - outputArtifactKey: tuning_metrics
            producerSubtask: tune-op
    tasks:
      preprocess-op:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-preprocess-op
        inputs:
          parameters:
            dataset_url:
              componentInputParameter: dataset_url
            random_state:
              componentInputParameter: random_state
        taskInfo:
          name: preprocess-op
      train-eval-torch-op:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-eval-torch-op
        dependentTasks:
        - preprocess-op
        - tune-op
        inputs:
          artifacts:
            best_params:
              taskOutputArtifact:
                outputArtifactKey: best_params
                producerTask: tune-op
            dataset:
              taskOutputArtifact:
                outputArtifactKey: output_dataset
                producerTask: preprocess-op
          parameters:
            epochs:
              componentInputParameter: train_epochs
            seed:
              componentInputParameter: random_state
        taskInfo:
          name: train-eval-torch-op
      tune-op:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-tune-op
        dependentTasks:
        - preprocess-op
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: output_dataset
                producerTask: preprocess-op
          parameters:
            epochs:
              componentInputParameter: tune_epochs
            seed:
              componentInputParameter: random_state
            trials:
              componentInputParameter: trials
        taskInfo:
          name: tune-op
  inputDefinitions:
    parameters:
      dataset_url:
        defaultValue: https://archive.ics.uci.edu/ml/machine-learning-databases/adult
        isOptional: true
        parameterType: STRING
      random_state:
        defaultValue: 42.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      train_epochs:
        defaultValue: 20.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      trials:
        defaultValue: 10.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      tune_epochs:
        defaultValue: 5.0
        isOptional: true
        parameterType: NUMBER_INTEGER
  outputDefinitions:
    artifacts:
      train-eval-torch-op-metrics_out:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      tune-op-tuning_metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.4.0
