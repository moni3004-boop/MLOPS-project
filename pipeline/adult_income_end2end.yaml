# PIPELINE DEFINITION
# Name: adult-income-end2end
# Inputs:
#    batch_size: int [Default: 256.0]
#    dataset_url: str [Default: 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult']
#    dropout: float [Default: 0.2]
#    epochs: int [Default: 20.0]
#    hidden: int [Default: 128.0]
#    lr: float [Default: 0.001]
#    random_state: int [Default: 42.0]
#    test_size: float [Default: 0.2]
#    val_size: float [Default: 0.1]
components:
  comp-preprocess-op:
    executorLabel: exec-preprocess-op
    inputDefinitions:
      parameters:
        dataset_url:
          parameterType: STRING
        random_state:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        test_size:
          defaultValue: 0.2
          isOptional: true
          parameterType: NUMBER_DOUBLE
        val_size:
          defaultValue: 0.1
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        output_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-eval-op:
    executorLabel: exec-train-eval-op
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        batch_size:
          defaultValue: 256.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        dropout:
          defaultValue: 0.2
          isOptional: true
          parameterType: NUMBER_DOUBLE
        epochs:
          defaultValue: 20.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        hidden:
          defaultValue: 128.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        lr:
          defaultValue: 0.001
          isOptional: true
          parameterType: NUMBER_DOUBLE
        random_state:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      artifacts:
        metrics_out:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-preprocess-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'numpy==1.26.4'\
          \ 'scikit-learn==1.4.2' 'joblib==1.4.2'  &&  python3 -m pip install --quiet\
          \ --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5;\
          \ python_version<\"3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess_op(\n    dataset_url: str,\n    output_dataset: Output[Dataset],\n\
          \    test_size: float = 0.2,\n    val_size: float = 0.1,\n    random_state:\
          \ int = 42,\n) -> None:\n    \"\"\"Download Adult dataset, preprocess (OHE\
          \ + scale), split train/val/test, save .npz to output_dataset.path.\"\"\"\
          \n    import csv\n    import os\n    import urllib.request\n    import numpy\
          \ as np\n    from sklearn.model_selection import train_test_split\n    from\
          \ sklearn.compose import ColumnTransformer\n    from sklearn.preprocessing\
          \ import OneHotEncoder, StandardScaler\n    from sklearn.pipeline import\
          \ Pipeline\n    import joblib\n\n    adult_data_url = dataset_url.rstrip(\"\
          /\") + \"/adult.data\"\n    adult_test_url = dataset_url.rstrip(\"/\") +\
          \ \"/adult.test\"\n\n    def _download_text(url: str) -> str:\n        with\
          \ urllib.request.urlopen(url) as resp:\n            return resp.read().decode(\"\
          utf-8\", errors=\"replace\")\n\n    def _parse_adult_csv(text: str, is_test:\
          \ bool) -> list[list[str]]:\n        lines: list[str] = []\n        for\
          \ i, raw in enumerate(text.splitlines()):\n            raw = raw.strip()\n\
          \            if not raw or raw.startswith(\"|\"):\n                continue\n\
          \            if is_test and i == 0 and raw.lower().startswith(\"age\"):\n\
          \                continue\n            lines.append(raw)\n\n        rows:\
          \ list[list[str]] = []\n        reader = csv.reader(lines, delimiter=\"\
          ,\", skipinitialspace=True)\n        for row in reader:\n            if\
          \ len(row) != 15:\n                continue\n            if is_test:\n \
          \               row[-1] = row[-1].rstrip(\".\")\n            rows.append(row)\n\
          \        return rows\n\n    data_text = _download_text(adult_data_url)\n\
          \    test_text = _download_text(adult_test_url)\n\n    train_rows = _parse_adult_csv(data_text,\
          \ is_test=False)\n    test_rows = _parse_adult_csv(test_text, is_test=True)\n\
          \n    all_rows = train_rows + test_rows\n    X_raw = [r[:-1] for r in all_rows]\n\
          \    y_raw = [r[-1] for r in all_rows]\n\n    y = np.array([1 if v.strip()\
          \ == \">50K\" else 0 for v in y_raw], dtype=np.int64)\n\n    # Numeric /\
          \ categorical indices (Adult dataset)\n    numeric_idx = [0, 2, 4, 10, 11,\
          \ 12]\n    categorical_idx = [1, 3, 5, 6, 7, 8, 9, 13]\n\n    preprocessor\
          \ = ColumnTransformer(\n        transformers=[\n            (\"num\", Pipeline(steps=[(\"\
          scaler\", StandardScaler())]), numeric_idx),\n            (\"cat\", OneHotEncoder(handle_unknown=\"\
          ignore\", sparse_output=False), categorical_idx),\n        ],\n        remainder=\"\
          drop\",\n    )\n\n    X_obj = np.array(X_raw, dtype=object)\n    X = preprocessor.fit_transform(X_obj).astype(np.float32)\n\
          \n    # Split train vs (val+test)\n    X_train, X_tmp, y_train, y_tmp =\
          \ train_test_split(\n        X,\n        y,\n        test_size=(test_size\
          \ + val_size),\n        random_state=random_state,\n        stratify=y,\n\
          \    )\n\n    # Split tmp into val/test\n    val_frac = val_size / max(1e-9,\
          \ (val_size + test_size))\n    X_val, X_test, y_val, y_test = train_test_split(\n\
          \        X_tmp,\n        y_tmp,\n        test_size=(1.0 - val_frac),\n \
          \       random_state=random_state,\n        stratify=y_tmp,\n    )\n\n \
          \   os.makedirs(output_dataset.path, exist_ok=True)\n\n    # Save arrays\
          \ compactly\n    np.savez_compressed(\n        os.path.join(output_dataset.path,\
          \ \"splits.npz\"),\n        X_train=X_train, y_train=y_train,\n        X_val=X_val,\
          \ y_val=y_val,\n        X_test=X_test, y_test=y_test,\n    )\n\n    # Save\
          \ preprocessor too (optional but useful)\n    joblib.dump(preprocessor,\
          \ os.path.join(output_dataset.path, \"preprocessor.joblib\"))\n\n"
        image: python:3.11-slim
    exec-train-eval-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_eval_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'numpy==1.26.4'\
          \ 'scikit-learn==1.4.2' 'joblib==1.4.2'  &&  python3 -m pip install --quiet\
          \ --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5;\
          \ python_version<\"3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_eval_op(\n    dataset: Input[Dataset],\n    model: Output[Model],\n\
          \    metrics_out: Output[Metrics],\n    epochs: int = 20,\n    batch_size:\
          \ int = 256,\n    lr: float = 1e-3,\n    hidden: int = 128,\n    dropout:\
          \ float = 0.2,\n    random_state: int = 42,\n) -> None:\n    \"\"\"Train\
          \ an MLPClassifier (sklearn) + log val/test metrics + save model.joblib.\"\
          \"\"\n    import os\n    import numpy as np\n    import joblib\n    from\
          \ sklearn.neural_network import MLPClassifier\n    from sklearn.metrics\
          \ import accuracy_score, f1_score, log_loss\n\n    data = np.load(os.path.join(dataset.path,\
          \ \"splits.npz\"))\n    X_train = data[\"X_train\"]; y_train = data[\"y_train\"\
          ]\n    X_val = data[\"X_val\"]; y_val = data[\"y_val\"]\n    X_test = data[\"\
          X_test\"]; y_test = data[\"y_test\"]\n\n    clf = MLPClassifier(\n     \
          \   hidden_layer_sizes=(hidden, hidden),\n        activation=\"relu\",\n\
          \        solver=\"adam\",\n        alpha=dropout,              # not true\
          \ dropout, but keeps your \"dropout\" knob meaningful\n        batch_size=batch_size,\n\
          \        learning_rate_init=lr,\n        max_iter=epochs,\n        early_stopping=True,\n\
          \        n_iter_no_change=5,\n        tol=1e-4,\n        random_state=random_state,\n\
          \        verbose=True,               # prints \"Iteration X, loss = ...\"\
          \n    )\n\n    clf.fit(X_train, y_train)\n\n    # Validation metrics\n \
          \   val_proba = clf.predict_proba(X_val)[:, 1]\n    val_pred = (val_proba\
          \ >= 0.5).astype(int)\n    val_acc = float(accuracy_score(y_val, val_pred))\n\
          \    val_f1 = float(f1_score(y_val, val_pred))\n    val_ll = float(log_loss(y_val,\
          \ val_proba))\n\n    # Test metrics\n    test_proba = clf.predict_proba(X_test)[:,\
          \ 1]\n    test_pred = (test_proba >= 0.5).astype(int)\n    test_acc = float(accuracy_score(y_test,\
          \ test_pred))\n    test_f1 = float(f1_score(y_test, test_pred))\n\n    #\
          \ Log metrics into the Metrics artifact metadata\n    metrics_out.log_metric(\"\
          val_accuracy\", val_acc)\n    metrics_out.log_metric(\"val_f1\", val_f1)\n\
          \    metrics_out.log_metric(\"val_log_loss\", val_ll)\n    metrics_out.log_metric(\"\
          test_accuracy\", test_acc)\n    metrics_out.log_metric(\"test_f1\", test_f1)\n\
          \n    # Save model artifact\n    os.makedirs(model.path, exist_ok=True)\n\
          \    joblib.dump(clf, os.path.join(model.path, \"model.joblib\"))\n\n"
        image: python:3.11-slim
pipelineInfo:
  name: adult-income-end2end
root:
  dag:
    tasks:
      preprocess-op:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-preprocess-op
        inputs:
          parameters:
            dataset_url:
              componentInputParameter: dataset_url
            random_state:
              componentInputParameter: random_state
            test_size:
              componentInputParameter: test_size
            val_size:
              componentInputParameter: val_size
        taskInfo:
          name: preprocess-op
      train-eval-op:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-eval-op
        dependentTasks:
        - preprocess-op
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: output_dataset
                producerTask: preprocess-op
          parameters:
            batch_size:
              componentInputParameter: batch_size
            dropout:
              componentInputParameter: dropout
            epochs:
              componentInputParameter: epochs
            hidden:
              componentInputParameter: hidden
            lr:
              componentInputParameter: lr
            random_state:
              componentInputParameter: random_state
        taskInfo:
          name: train-eval-op
  inputDefinitions:
    parameters:
      batch_size:
        defaultValue: 256.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      dataset_url:
        defaultValue: https://archive.ics.uci.edu/ml/machine-learning-databases/adult
        isOptional: true
        parameterType: STRING
      dropout:
        defaultValue: 0.2
        isOptional: true
        parameterType: NUMBER_DOUBLE
      epochs:
        defaultValue: 20.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      hidden:
        defaultValue: 128.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      lr:
        defaultValue: 0.001
        isOptional: true
        parameterType: NUMBER_DOUBLE
      random_state:
        defaultValue: 42.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      test_size:
        defaultValue: 0.2
        isOptional: true
        parameterType: NUMBER_DOUBLE
      val_size:
        defaultValue: 0.1
        isOptional: true
        parameterType: NUMBER_DOUBLE
schemaVersion: 2.1.0
sdkVersion: kfp-2.15.2
